{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_generator[Videos].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpgL7zXJ8hQpnFgIvzgunw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpraveenkanna/Style_generator-Video_and_Image/blob/master/Style_generator%5BVideos%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbJSNKv1YN9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "import requests\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "urllib.request.urlretrieve('http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz', download_path)\n",
        "\n",
        "#Loading model\n",
        "class DeepLabModel(object):\n",
        "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "  INPUT_SIZE = 513\n",
        "  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "  def __init__(self, tarball_path):\n",
        "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "    self.graph = tf.Graph()\n",
        "\n",
        "    graph_def = None\n",
        "    # Extract frozen graph from tar archive.\n",
        "    tar_file = tarfile.open(tarball_path)\n",
        "    for tar_info in tar_file.getmembers():\n",
        "      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "        file_handle = tar_file.extractfile(tar_info)\n",
        "        graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "        break\n",
        "\n",
        "    tar_file.close()\n",
        "\n",
        "    if graph_def is None:\n",
        "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "    with self.graph.as_default():\n",
        "      tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "  def run_inferance(self, image):\n",
        "\n",
        "    \n",
        "    batch_seg_map = self.sess.run(\n",
        "        self.OUTPUT_TENSOR_NAME,\n",
        "        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(image)]})\n",
        "    seg_map = batch_seg_map[0]\n",
        "    return image, seg_map\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdaDUi2JZl2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_result(input_image,style_image):\n",
        "  input_image_copy_for_requests = input_image\n",
        "\n",
        "  #Preprocessing image\n",
        "  original_height = input_image.shape[0]\n",
        "  original_width =input_image.shape[1]\n",
        "  original_resolution = (original_width,original_height)\n",
        "\n",
        "  #resizing for model inputs\n",
        "  INPUT_SIZE= 513\n",
        "  height = input_image.shape[0]\n",
        "  width =input_image.shape[1]\n",
        "  resize_ratio = 1.0 * INPUT_SIZE / max(width, height)\n",
        "  target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "  input_image = cv2.resize(input_image,target_size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "  style_image = cv2.normalize(src=style_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  style_image = cv2.resize(style_image,target_size,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  height = input_image.shape[0]\n",
        "  width =input_image.shape[1]\n",
        "\n",
        "  \n",
        "\n",
        "  #Extract persons in the image\n",
        "  _, seg_map = MODEL.run_inferance(input_image)\n",
        "  person_seg = seg_map\n",
        "  np.place(person_seg, person_seg != 15, 0)\n",
        "  mask = np.dstack((person_seg, person_seg,person_seg))\n",
        "  img_background = (mask)*input_image \n",
        "  img_background = cv2.normalize(src=img_background, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  person_extracted = cv2.resize(img_background,(width,height),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "  #cv2_imshow(person_extracted)\n",
        "\n",
        "  #get Mask\n",
        "  _, seg_map = MODEL.run_inferance(input_image)\n",
        "  getMask = seg_map\n",
        "  np.place(getMask, getMask != 15, 1) \n",
        "  np.place(getMask, getMask == 15, 0) \n",
        "\n",
        "  img_background = cv2.normalize(src=getMask, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  mask = cv2.resize(img_background,(width,height),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "  #cv2_imshow(mask)\n",
        "\n",
        "  # apply style \n",
        "  raw_mask = np.dstack((getMask, getMask,getMask))\n",
        "  extract_background = (style_image)* raw_mask\n",
        "  extract_background1 = cv2.normalize(src=extract_background, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  background = cv2.resize(extract_background1,(width,height),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "  #cv2_imshow(background)\n",
        "\n",
        "  extract_person = (style_image)* (1-raw_mask)\n",
        "  extract_person1 = cv2.normalize(src=extract_person, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  peson1 = cv2.resize(extract_person1,(width,height),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "  #cv2_imshow(peson1)\n",
        "\n",
        "\n",
        "  #style background\n",
        "  stylized_pic = (extract_background)+person_extracted #background stylized\n",
        "  stylized_pic_normalized = cv2.normalize(src=stylized_pic, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  stylized_pic_view = cv2.resize(stylized_pic_normalized,original_resolution,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  ##cv2_imshow(stylized_pic_view)\n",
        "\n",
        "  #Style person\n",
        "  stylized_pic_person = (input_image * (raw_mask))+(extract_person) #person stylized\n",
        "  stylized_pic_person_normalized = cv2.normalize(src=stylized_pic_person, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  stylized_pic_person_view = cv2.resize(stylized_pic_person_normalized,original_resolution,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  ##cv2_imshow(stylized_pic_person_view)\n",
        "\n",
        "  #Person styled - light\n",
        "  stylized_img_light = (extract_person* (1-raw_mask))+input_image #person stylized\n",
        "\n",
        "  stylized_img_light_normalized = cv2.normalize(src=stylized_img_light, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "  stylized_img_light_normalized_view = cv2.resize(stylized_img_light_normalized,original_resolution,fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  ##cv2_imshow(stylized_img_light_normalized_view)\n",
        "  final_images_packed = json.dumps({\"input_image\": input_image_copy_for_requests.tolist(),\n",
        "                           \"background_styled\":stylized_pic_view.tolist(),\n",
        "                           \"person_styled\": stylized_pic_person_view.tolist(),\n",
        "                           \"stylized_img_light\": stylized_img_light_normalized_view.tolist()\n",
        "                           })\n",
        "  #print(\"success get_result\" )\n",
        "  return final_images_packed\n",
        "\n",
        "\n",
        "def process_style(source_style,input_image,url):\n",
        "  data = json.dumps({\"style_image\": source_style.tolist(),\n",
        "                   \"source_image\": input_image.tolist()})\n",
        "\n",
        "  headers = {'content-type': 'application/json'}\n",
        "  response = requests.post(url, json = data, headers = headers)\n",
        "  loading_data = response.json()\n",
        "  style_image = np.asarray(loading_data[\"style_image\"])\n",
        "  return style_image\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgS-5ZbyZ6MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%%time\n",
        "input_source = \"/content/input.mp4\"  #Input video\n",
        "source_style = cv2.imread(\"/content/style8.jpg\") # Style need to applied in the form of image\n",
        "\n",
        "\"\"\"\n",
        "Run Style_Transfer_Model.ipynb in seperate session.\n",
        "\n",
        "Get the API URL to make POST request \n",
        "\n",
        "\"\"\"\n",
        "url = \"http://a0a38dc5.ngrok.io/style\"\n",
        "\n",
        "counter=0\n",
        "cap = cv2.VideoCapture(input_source)\n",
        "res_width = int(cap.get(3))\n",
        "res_height = int(cap.get(4))\n",
        "no_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(\"Total number of frames\",no_of_frames )\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID') #saving format\n",
        "out = cv2.VideoWriter('styled_output.mp4', fourcc, 30, (res_width,res_height)) #To save video\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    input_image = frame\n",
        "    \n",
        "\n",
        "    try:\n",
        "      style_image = process_style(source_style,input_image,url) #model 1    \n",
        "      response = get_result(input_image,style_image) #model 2\n",
        "      results = json.loads(response)\n",
        "      #dict_keys(['input_image', 'background_styled', 'person_styled', 'stylized_img_light'])\n",
        "      output = np.asarray(results[\"stylized_img_light\"])\n",
        "      output = cv2.normalize(src=output, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "\n",
        "    \n",
        "    except Exception as e:\n",
        "      print(\"Error\",e)\n",
        "      break\n",
        "\n",
        "    out.write(output)\n",
        "\n",
        "    counter+=1\n",
        "    print(\"Completed\",counter,\"of\",no_of_frames)\n",
        "   \n",
        "\n",
        "    key = cv2.waitKey(30)\n",
        "    esc_code = 27\n",
        "    if key == esc_code:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\"success\")\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}